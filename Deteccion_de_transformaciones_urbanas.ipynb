{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GiHx3dghXJFq",
        "-OAgtC7YXU0w",
        "FIuJEvVQXo1R",
        "VFoE9CVcYQzB",
        "yk6-5aMCYk_z",
        "CuSg5x0rYg7u"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencias"
      ],
      "metadata": {
        "id": "GiHx3dghXJFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com"
      ],
      "metadata": {
        "id": "9QRwqSAcXQkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio"
      ],
      "metadata": {
        "id": "_WLjMS4RXQJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3vpUPWoW4ji"
      },
      "outputs": [],
      "source": [
        "# Librerias\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.features import rasterize\n",
        "from shapely.geometry import Polygon\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.metrics import (\n",
        "    adjusted_rand_score,\n",
        "    adjusted_mutual_info_score,\n",
        "    classification_report,\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    jaccard_score\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.manifold import TSNE\n",
        "from cuml.naive_bayes import GaussianNB\n",
        "from cuml.svm import SVC\n",
        "from cuml.ensemble import RandomForestClassifier as cuRF\n",
        "from cuml.metrics import accuracy_score as cu_accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import cv2\n",
        "from scipy.ndimage import uniform_filter\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ohq1JcsuXiHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features"
      ],
      "metadata": {
        "id": "-OAgtC7YXU0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ðŸ“ RUTAS\n",
        "# ==========================================\n",
        "base_dir = \"/content/drive/MyDrive/Imagenes\"\n",
        "mask_dir = os.path.join(base_dir, \"Recortes_Mask\")\n",
        "rgb_dir  = os.path.join(base_dir, \"Recortes_RGB\")\n",
        "\n",
        "print(\"ðŸ“‚ Carpetas detectadas:\")\n",
        "print(\"MASKs:\", len(glob.glob(os.path.join(mask_dir, \"*.tiff\"))))\n",
        "print(\"RGBs :\", len(glob.glob(os.path.join(rgb_dir, \"*.tiff\"))))\n",
        "\n",
        "# ==========================================\n",
        "# ðŸ§  FUNCIÃ“N: extraer caracterÃ­sticas (RGB + Ã­ndices)\n",
        "# ==========================================\n",
        "\n",
        "def extraer_features(img):\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    R, G, B = img[..., 0], img[..., 1], img[..., 2]\n",
        "\n",
        "    # --- Calcular medias locales (como ya hacÃ­as) ---\n",
        "    R_mean = uniform_filter(R, size=3)\n",
        "    G_mean = uniform_filter(G, size=3)\n",
        "    B_mean = uniform_filter(B, size=3)\n",
        "\n",
        "    # --- Combinar caracterÃ­sticas ---\n",
        "    features = np.stack([\n",
        "        R, G, B,\n",
        "        R_mean, G_mean, B_mean\n",
        "    ], axis=-1)\n",
        "\n",
        "    return features\n",
        "\n",
        "# ==========================================\n",
        "# ðŸ“š CARGA Y PREPARACIÃ“N DE DATOS\n",
        "# ==========================================\n",
        "X_data, y_data = [], []\n",
        "\n",
        "mask_files = sorted(glob.glob(os.path.join(mask_dir, \"*.tiff\")))\n",
        "\n",
        "for mask_path in tqdm(mask_files):\n",
        "    name = os.path.splitext(os.path.basename(mask_path))[0]\n",
        "    rgb_path = os.path.join(rgb_dir, name + \".tiff\")\n",
        "\n",
        "    if not os.path.exists(rgb_path):\n",
        "        print(f\"âš ï¸ No se encontrÃ³ RGB para {name}\")\n",
        "        continue\n",
        "\n",
        "    # --- Cargar imagen RGB ---\n",
        "    with rasterio.open(rgb_path) as src:\n",
        "        img = src.read().transpose(1, 2, 0)\n",
        "        img_shape = img.shape[:2]\n",
        "\n",
        "    # --- Cargar mÃ¡scara ---\n",
        "    with rasterio.open(mask_path) as src:\n",
        "        mask = src.read(1)\n",
        "\n",
        "    # --- Extraer caracterÃ­sticas ---\n",
        "    feats = extraer_features(img)\n",
        "    X = feats.reshape(-1, feats.shape[2])\n",
        "    y = (mask > 0).astype(np.uint8).flatten()\n",
        "\n",
        "    # --- Muestreo balanceado ---\n",
        "    pos_idx = np.where(y == 1)[0]\n",
        "    neg_idx = np.where(y == 0)[0]\n",
        "    if len(pos_idx) == 0 or len(neg_idx) == 0:\n",
        "        continue\n",
        "    n_samples = min(len(pos_idx), len(neg_idx), 10000)\n",
        "    sample_idx = np.concatenate([\n",
        "        np.random.choice(pos_idx, n_samples, replace=False),\n",
        "        np.random.choice(neg_idx, n_samples, replace=False)\n",
        "    ])\n",
        "\n",
        "    X_data.append(X[sample_idx])\n",
        "    y_data.append(y[sample_idx])\n",
        "\n",
        "print(f\"ðŸ“Š ImÃ¡genes procesadas: {len(X_data)}\")\n",
        "\n",
        "if len(X_data) == 0:\n",
        "    raise ValueError(\"âŒ No se generÃ³ ningÃºn conjunto de entrenamiento. Revisa los nombres o las rutas.\")\n",
        "\n",
        "# --- Concatenar todo ---\n",
        "X_data = np.concatenate(X_data)\n",
        "y_data = np.concatenate(y_data)\n",
        "print(\"âœ… Dataset final:\", X_data.shape, y_data.shape)"
      ],
      "metadata": {
        "id": "GWOT_jtuXT-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML: Tradicional"
      ],
      "metadata": {
        "id": "FIuJEvVQXo1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RandomForest\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
        "rf = cuRF(n_estimators=20, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ðŸŽ¯ IoU:\", jaccard_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "XteiRW2TXtUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gaussian Naive Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred_gnb = gnb.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_gnb))\n",
        "print(\"ðŸŽ¯ IoU:\", jaccard_score(y_test, y_pred_gnb))"
      ],
      "metadata": {
        "id": "vBjo4zrdX_zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Desicion Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "print(\"ðŸŽ¯ IoU:\", jaccard_score(y_test, y_pred_dt))"
      ],
      "metadata": {
        "id": "n3tLxjRKYAff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML: Deep Learning"
      ],
      "metadata": {
        "id": "VFoE9CVcYQzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
        ")\n",
        "\n",
        "# Normalizamos los valores (muy importante para redes neuronales)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# ==========================================\n",
        "# ðŸ§  MODELO DE RED NEURONAL DENSA\n",
        "# ==========================================\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # salida binaria\n",
        "])\n",
        "\n",
        "# ==========================================\n",
        "# âš™ï¸ COMPILAR EL MODELO\n",
        "# ==========================================\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# ðŸš€ ENTRENAMIENTO\n",
        "# ==========================================\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss', patience=5, restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10,\n",
        "    batch_size=256,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# ðŸ§¾ EVALUACIÃ“N\n",
        "# ==========================================\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ðŸŽ¯ IoU:\", jaccard_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "KRimmm1PYVLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# No supervisado"
      ],
      "metadata": {
        "id": "yk6-5aMCYk_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# En feature tomar solo de 1 a 10 imagenes para que el rendimiento sea optimo\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.20, random_state=42)\n",
        "\n",
        "n_features = X_train.shape[1]\n",
        "n_components = max(1, n_features // 2)\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "pca.fit(X_train)\n",
        "\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca  = pca.transform(X_test)\n",
        "\n",
        "X_train_concat = np.concatenate([X_train, X_train_pca], axis=1)\n",
        "X_test_concat  = np.concatenate([X_test, X_test_pca], axis=1)\n",
        "\n",
        "\n",
        "def build_dnn(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_dim,)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=SGD(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_and_eval(Xtr, Xte, title):\n",
        "    print(\"\\n======================================\")\n",
        "    print(\"MODELO:\", title)\n",
        "    print(\"======================================\")\n",
        "\n",
        "    model = build_dnn(Xtr.shape[1])\n",
        "\n",
        "    model.fit(Xtr, y_train,validation_data=(Xte, y_test),epochs=3,batch_size=256,verbose=0)\n",
        "\n",
        "    # Train predictions\n",
        "    y_pred_train = (model.predict(Xtr) > 0.5).astype(int)\n",
        "    acc_train = accuracy_score(y_train, y_pred_train)\n",
        "    f1_train  = f1_score(y_train, y_pred_train)\n",
        "\n",
        "    # Test predictions\n",
        "    y_pred_test = (model.predict(Xte) > 0.5).astype(int)\n",
        "    acc_test = accuracy_score(y_test, y_pred_test)\n",
        "    f1_test  = f1_score(y_test, y_pred_test)\n",
        "\n",
        "    print(f\"Train Accuracy: {acc_train:.4f} | Train F1: {f1_train:.4f}\")\n",
        "    print(f\"Test  Accuracy: {acc_test:.4f} | Test  F1: {f1_test:.4f}\")\n",
        "\n",
        "    return (acc_train, f1_train, acc_test, f1_test)\n",
        "\n",
        "# --- 1 Solo Features originales\n",
        "res_orig = train_and_eval(X_train, X_test, \"Solo FEAT\")\n",
        "\n",
        "# --- 2ï¸ Features + PCA\n",
        "res_concat = train_and_eval(X_train_concat, X_test_concat, \"FEAT + PCA\")\n",
        "\n",
        "# --- 3ï¸ Solo PCA\n",
        "res_pca = train_and_eval(X_train_pca, X_test_pca, \"Solo PCA\")"
      ],
      "metadata": {
        "id": "5SgeolpAYoY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ãrbol de decisiÃ³n en un subconjunto\n",
        "X_sub, _, y_sub, _ = train_test_split(X_data, y_data, test_size=0.8, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
        "dt.fit(X_sub, y_sub)\n",
        "\n",
        "importances = dt.feature_importances_\n",
        "top2 = np.argsort(importances)[::-1][:2]\n",
        "\n",
        "print(\"\\nTop 2 caracterÃ­sticas mÃ¡s importantes del Ã¡rbol:\")\n",
        "print(\"Indices:\", top2.tolist())\n",
        "\n",
        "# PCA con 2 componentes\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_data)\n",
        "\n",
        "\n",
        "# TSNE con 2 componentes\n",
        "tsne = TSNE(n_components=2, random_state=0, perplexity=30)\n",
        "X_tsne = tsne.fit_transform(X_data)\n",
        "\n",
        "# 4. Normalizar + KMeans y DBSCAN\n",
        "scaler = StandardScaler()\n",
        "X_norm = scaler.fit_transform(X_data)\n",
        "\n",
        "# KMeans\n",
        "kmeans = KMeans(n_clusters=len(np.unique(y_data)), random_state=0)\n",
        "kmeans_labels = kmeans.fit_predict(X_norm)\n",
        "\n",
        "# DBSCAN\n",
        "dbscan = DBSCAN(eps=0.7, min_samples=20, n_jobs=-1)\n",
        "db_labels = dbscan.fit_predict(X_norm)\n",
        "\n",
        "# 5. FunciÃ³n de scatter\n",
        "def scatter(ax, data, labels, title):\n",
        "    ax.scatter(data[:,0], data[:,1], s=8, c=labels, cmap=\"tab10\")\n",
        "    ax.set_title(title)\n",
        "\n",
        "# 6. FIGURA 3Ã—3 (CON TSNE, SIN SUBMUESTREO)\n",
        "fig, axes = plt.subplots(3, 3, figsize=(14, 14))\n",
        "\n",
        "# ---- Fila 1  PCA ----\n",
        "scatter(axes[0,0], X_pca, kmeans_labels, \"PCA - KMeans\")\n",
        "scatter(axes[0,1], X_pca, db_labels, \"PCA - DBSCAN\")\n",
        "scatter(axes[0,2], X_pca, y_data, \"PCA - Clases Dataset\")\n",
        "\n",
        "# ---- Fila 2  TSNE ----\n",
        "scatter(axes[1,0], X_tsne, kmeans_labels, \"TSNE - KMeans\")\n",
        "scatter(axes[1,1], X_tsne, db_labels, \"TSNE - DBSCAN\")\n",
        "scatter(axes[1,2], X_tsne, y_data, \"TSNE - Clases Dataset\")\n",
        "\n",
        "# ---- Fila 3  Mejores 2 features ----\n",
        "X_best2 = X_data[:, top2]\n",
        "scatter(axes[2,0], X_best2, kmeans_labels, \"Best2 - KMeans\")\n",
        "scatter(axes[2,1], X_best2, db_labels, \"Best2 - DBSCAN\")\n",
        "scatter(axes[2,2], X_best2, y_data, \"Best2 - Clases Dataset\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WGZSGvN6Ys6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruebas"
      ],
      "metadata": {
        "id": "CuSg5x0rYg7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ðŸ“¸ CARGAR IMAGEN Y MÃSCARA\n",
        "# ==========================================\n",
        "\n",
        "test_img_path = sorted(glob.glob(os.path.join(rgb_dir, \"*.tiff\")))[13]\n",
        "test_mask_path = sorted(glob.glob(os.path.join(mask_dir, \"*.tiff\")))[13]\n",
        "\n",
        "with rasterio.open(test_img_path) as src:\n",
        "    img_test = src.read().transpose(1, 2, 0)\n",
        "\n",
        "with rasterio.open(test_mask_path) as src:\n",
        "    mask_test = src.read(1)\n",
        "\n",
        "# ==========================================\n",
        "# ðŸ§© EXTRAER FEATURES (debe coincidir con entrenamiento)\n",
        "# ==========================================\n",
        "img_feats = extraer_features(img_test).reshape(-1, 6)\n",
        "img_feats = img_feats / 255.0  # normalizar igual que en entrenamiento\n",
        "\n",
        "# ==========================================\n",
        "# ðŸ§  PREDICCIÃ“N CON LA RED NEURONAL\n",
        "# ==========================================\n",
        "mask_pred_prob = model.predict(img_feats, batch_size=4096, verbose=1)\n",
        "mask_pred = (mask_pred_prob > 0.5).astype(np.uint8).reshape(img_test.shape[:2])\n",
        "\n",
        "# ==========================================\n",
        "# ðŸŽ¨ VISUALIZACIÃ“N\n",
        "# ==========================================\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Imagen original\")\n",
        "plt.imshow(img_test)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"MÃ¡scara predicha\")\n",
        "plt.imshow(mask_pred, cmap='gray')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"MÃ¡scara real (Ground Truth)\")\n",
        "plt.imshow(mask_test, cmap='gray')\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sM31OWQfYgNB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}